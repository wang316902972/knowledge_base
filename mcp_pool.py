#!/usr/bin/env python3
"""
MCPè¿æ¥æ± å’Œç¼“å­˜ç®¡ç†
æä¾›é«˜æ€§èƒ½çš„MCPå®¢æˆ·ç«¯å¤ç”¨ã€æŸ¥è¯¢ç¼“å­˜å’Œæ‰¹é‡æŸ¥è¯¢åŠŸèƒ½
"""

import json
import asyncio
import hashlib
import logging
from typing import Dict, Any, Optional, List
from functools import lru_cache
from datetime import datetime, timedelta

# ä¼˜å…ˆä½¿ç”¨æ–°çš„æœç´¢å®¢æˆ·ç«¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨æ—§çš„MCPå®¢æˆ·ç«¯
try:
    from mcp_search_client import MCPSearchClient as MCPClient
    logger = logging.getLogger(__name__)
    logger.info("âœ… ä½¿ç”¨æ–°çš„MCPæœç´¢å®¢æˆ·ç«¯")

    # åˆ›å»ºé€‚é…å™¨å‡½æ•°ï¼Œå°† mcp_url å‚æ•°æ˜ å°„åˆ° search_url
    def create_mcp_client(mcp_url: str, client_name: str = "mcp-pool-client", **kwargs) -> MCPClient:
        """
        åˆ›å»ºMCPå®¢æˆ·ç«¯çš„é€‚é…å™¨å‡½æ•°

        Args:
            mcp_url: æœç´¢æœåŠ¡åœ°å€
            client_name: å®¢æˆ·ç«¯åç§°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            MCPClient: å®¢æˆ·ç«¯å®ä¾‹
        """
        from mcp_search_client import create_mcp_search_client
        return create_mcp_search_client(
            search_url=mcp_url,
            client_name=client_name,
            **kwargs
        )

except ImportError:
    from mcp_client import MCPClient, create_mcp_client
    logger = logging.getLogger(__name__)
    logger.warning("âš ï¸ é™çº§ä½¿ç”¨æ—§çš„MCPå®¢æˆ·ç«¯")


class MCPConnectionPool:
    """MCPè¿æ¥æ± ç®¡ç†ç±»"""

    def __init__(self,
                 mcp_url: str,
                 pool_size: int = 3,
                 client_name: str = "mcp-pool-client"):
        """
        åˆå§‹åŒ–è¿æ¥æ± 

        Args:
            mcp_url: MCPæœåŠ¡åœ°å€
            pool_size: è¿æ¥æ± å¤§å°
            client_name: å®¢æˆ·ç«¯åç§°
        """
        self.mcp_url = mcp_url
        self.pool_size = pool_size
        self.client_name = client_name
        self._pool: List[MCPClient] = []
        self._current_index = 0
        self._lock = asyncio.Lock()

    async def get_client(self) -> MCPClient:
        """
        ä»è¿æ¥æ± è·å–å®¢æˆ·ç«¯

        Returns:
            MCPClient: å¯ç”¨çš„å®¢æˆ·ç«¯å®ä¾‹
        """
        async with self._lock:
            # å¦‚æœæ± æœªæ»¡,åˆ›å»ºæ–°å®¢æˆ·ç«¯
            if len(self._pool) < self.pool_size:
                client = create_mcp_client(
                    mcp_url=self.mcp_url,
                    client_name=f"{self.client_name}-{len(self._pool)}"
                )
                # å¼‚æ­¥åˆå§‹åŒ–
                if await client.initialize_async():
                    self._pool.append(client)
                    logger.info(f"åˆ›å»ºæ–°MCPå®¢æˆ·ç«¯,å½“å‰æ± å¤§å°: {len(self._pool)}")
                    return client
                else:
                    logger.error("MCPå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")

            # ä½¿ç”¨è½®è¯¢ç­–ç•¥åˆ†é…å®¢æˆ·ç«¯
            if self._pool:
                client = self._pool[self._current_index % len(self._pool)]
                self._current_index += 1
                return client

            # é™çº§:åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯
            logger.warning("è¿æ¥æ± ä¸ºç©º,åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯")
            client = create_mcp_client(mcp_url=self.mcp_url)
            await client.initialize_async()
            return client

    async def close_all(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        async with self._lock:
            for client in self._pool:
                try:
                    await client.close_async()
                except Exception as e:
                    logger.error(f"å…³é—­å®¢æˆ·ç«¯å¤±è´¥: {e}")
            self._pool.clear()
            logger.info("æ‰€æœ‰MCPè¿æ¥å·²å…³é—­")


class MCPCache:
    """MCPæŸ¥è¯¢ç¼“å­˜ç®¡ç†ç±»"""

    def __init__(self, ttl_seconds: int = 3600, max_size: int = 1000):
        """
        åˆå§‹åŒ–ç¼“å­˜

        Args:
            ttl_seconds: ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        """
        self.ttl_seconds = ttl_seconds
        self.max_size = max_size
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._access_time: Dict[str, datetime] = {}
        self._lock = asyncio.Lock()

    def _generate_key(self, question: str, top_k: int) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{question}:{top_k}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()

    async def get(self, question: str, top_k: int) -> Optional[Dict[str, Any]]:
        """
        ä»ç¼“å­˜è·å–ç»“æœ

        Args:
            question: æœç´¢é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            ç¼“å­˜çš„ç»“æœ,å¦‚æœä¸å­˜åœ¨æˆ–å·²è¿‡æœŸè¿”å›None
        """
        cache_key = self._generate_key(question, top_k)

        async with self._lock:
            if cache_key not in self._cache:
                return None

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            cache_time = self._access_time.get(cache_key)
            if cache_time and datetime.now() - cache_time > timedelta(seconds=self.ttl_seconds):
                # ç¼“å­˜è¿‡æœŸ,åˆ é™¤
                del self._cache[cache_key]
                del self._access_time[cache_key]
                logger.info(f"ç¼“å­˜è¿‡æœŸå¹¶åˆ é™¤: {question[:50]}...")
                return None

            # æ›´æ–°è®¿é—®æ—¶é—´
            self._access_time[cache_key] = datetime.now()
            logger.info(f"ç¼“å­˜å‘½ä¸­: {question[:50]}...")
            return self._cache[cache_key]

    async def set(self, question: str, top_k: int, result: Dict[str, Any]):
        """
        è®¾ç½®ç¼“å­˜

        Args:
            question: æœç´¢é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            result: æŸ¥è¯¢ç»“æœ
        """
        cache_key = self._generate_key(question, top_k)

        async with self._lock:
            # LRUæ·˜æ±°ç­–ç•¥
            if len(self._cache) >= self.max_size:
                # æ‰¾åˆ°æœ€ä¹…æœªä½¿ç”¨çš„æ¡ç›®
                oldest_key = min(self._access_time.keys(),
                                key=lambda k: self._access_time[k])
                del self._cache[oldest_key]
                del self._access_time[oldest_key]
                logger.info(f"ç¼“å­˜å·²æ»¡,æ·˜æ±°æœ€ä¹…æœªä½¿ç”¨æ¡ç›®")

            self._cache[cache_key] = result
            self._access_time[cache_key] = datetime.now()
            logger.info(f"ç¼“å­˜å·²è®¾ç½®: {question[:50]}...")

    async def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        async with self._lock:
            self._cache.clear()
            self._access_time.clear()
            logger.info("ç¼“å­˜å·²æ¸…ç©º")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "size": len(self._cache),
            "max_size": self.max_size,
            "ttl_seconds": self.ttl_seconds
        }


class MCPQueryOptimizer:
    """MCPæŸ¥è¯¢ä¼˜åŒ–å™¨ - æä¾›æ‰¹é‡æŸ¥è¯¢å’Œæ™ºèƒ½ä¼˜åŒ–"""

    def __init__(self,
                 mcp_url: str,
                 pool_size: int = 3,
                 cache_ttl: int = 3600,
                 cache_max_size: int = 1000):
        """
        åˆå§‹åŒ–æŸ¥è¯¢ä¼˜åŒ–å™¨

        Args:
            mcp_url: MCPæœåŠ¡åœ°å€
            pool_size: è¿æ¥æ± å¤§å°
            cache_ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)
            cache_max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        """
        self.mcp_url = mcp_url
        self.pool = MCPConnectionPool(mcp_url, pool_size)
        self.cache = MCPCache(cache_ttl, cache_max_size)

    async def search_single(self,
                           question: str,
                           top_k: int = 5,
                           use_cache: bool = True,
                           timeout: float = 5.0) -> str:
        """
        å•æ¬¡æŸ¥è¯¢(å¸¦ç¼“å­˜å’Œè¶…æ—¶æ§åˆ¶)

        Args:
            question: æœç´¢é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            timeout: è¶…æ—¶æ—¶é—´(ç§’)

        Returns:
            str: JSONæ ¼å¼çš„æœç´¢ç»“æœ
        """
        # å°è¯•ä»ç¼“å­˜è·å–
        if use_cache:
            cached_result = await self.cache.get(question, top_k)
            if cached_result is not None:
                return json.dumps(cached_result, ensure_ascii=False)

        try:
            # ä½¿ç”¨è¿æ¥æ± è·å–å®¢æˆ·ç«¯
            client = await self.pool.get_client()

            # å¸¦è¶…æ—¶çš„æŸ¥è¯¢
            result = await asyncio.wait_for(
                client.search_async(question, top_k),
                timeout=timeout
            )

            # ç¼“å­˜ç»“æœ
            if use_cache and isinstance(result, dict):
                await self.cache.set(question, top_k, result)

            logger.info(f"æŸ¥è¯¢æˆåŠŸ: {question[:50]}...")
            return json.dumps(result, ensure_ascii=False)

        except asyncio.TimeoutError:
            logger.error(f"æŸ¥è¯¢è¶…æ—¶({timeout}ç§’): {question[:50]}...")
            return json.dumps({
                "status": "timeout",
                "message": f"MCPæœåŠ¡å“åº”è¶…æ—¶({timeout}ç§’)",
                "fallback_mode": "ä½¿ç”¨ç»éªŒä¼°ç®—",
                "results": []
            }, ensure_ascii=False)

        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¼‚å¸¸: {e}, é—®é¢˜: {question[:50]}...")
            return json.dumps({
                "status": "error",
                "message": str(e),
                "fallback_mode": "ä½¿ç”¨ç»éªŒä¼°ç®—",
                "results": []
            }, ensure_ascii=False)

    async def search_batch(self,
                          questions: List[str],
                          top_k: int = 5,
                          use_cache: bool = True,
                          timeout: float = 5.0,
                          max_concurrency: int = 5) -> List[str]:
        """
        æ‰¹é‡å¹¶å‘æŸ¥è¯¢

        Args:
            questions: é—®é¢˜åˆ—è¡¨
            top_k: è¿”å›ç»“æœæ•°é‡
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            timeout: å•ä¸ªæŸ¥è¯¢è¶…æ—¶æ—¶é—´(ç§’)
            max_concurrency: æœ€å¤§å¹¶å‘æ•°

        Returns:
            List[str]: JSONæ ¼å¼çš„æœç´¢ç»“æœåˆ—è¡¨
        """
        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrency)

        async def search_with_semaphore(question: str) -> str:
            async with semaphore:
                return await self.search_single(
                    question=question,
                    top_k=top_k,
                    use_cache=use_cache,
                    timeout=timeout
                )

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
        tasks = [search_with_semaphore(q) for q in questions]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸ç»“æœ
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"æ‰¹é‡æŸ¥è¯¢ç¬¬{i+1}é¡¹å¤±è´¥: {result}")
                formatted_results.append(json.dumps({
                    "status": "error",
                    "message": str(result),
                    "fallback_mode": "ä½¿ç”¨ç»éªŒä¼°ç®—",
                    "results": []
                }, ensure_ascii=False))
            else:
                formatted_results.append(result)

        logger.info(f"æ‰¹é‡æŸ¥è¯¢å®Œæˆ: {len(questions)}ä¸ªé—®é¢˜")
        return formatted_results

    async def close(self):
        """å…³é—­ä¼˜åŒ–å™¨"""
        await self.pool.close_all()
        await self.cache.clear()


# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
_global_optimizer: Optional[MCPQueryOptimizer] = None


def get_optimizer(mcp_url: Optional[str] = None) -> MCPQueryOptimizer:
    """
    è·å–å…¨å±€æŸ¥è¯¢ä¼˜åŒ–å™¨å®ä¾‹

    Args:
        mcp_url: MCPæœåŠ¡åœ°å€,é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–

    Returns:
        MCPQueryOptimizer: æŸ¥è¯¢ä¼˜åŒ–å™¨å®ä¾‹
    """
    global _global_optimizer

    if _global_optimizer is None:
        import os
        
        # ä¼˜å…ˆä½¿ç”¨æ–°çš„æœç´¢æœåŠ¡åœ°å€
        mcp_url = mcp_url or os.getenv(
            "KNOWLEDGE_BASE_SEARCH_URL",
            os.getenv(
                "MCP_URL",
                "http://192.168.244.189:8003/search"  # æ–°çš„é»˜è®¤åœ°å€
            )
        )

        logger.info(f"ğŸ“¡ çŸ¥è¯†åº“æœç´¢æœåŠ¡åœ°å€: {mcp_url}")
        logger.info(f"ğŸ’¡ æç¤º: å¦‚æœè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
        logger.info(f"   1. æœç´¢æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ")
        logger.info(f"   2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        logger.info(f"   3. URLé…ç½®æ˜¯å¦æ­£ç¡®")
        logger.info(f"   4. å¯é€šè¿‡ export KNOWLEDGE_BASE_SEARCH_URL=your_url è®¾ç½®è‡ªå®šä¹‰åœ°å€")

        _global_optimizer = MCPQueryOptimizer(
            mcp_url=mcp_url,
            pool_size=3,  # è¿æ¥æ± å¤§å°
            cache_ttl=3600,  # ç¼“å­˜1å°æ—¶
            cache_max_size=1000  # æœ€å¤šç¼“å­˜1000æ¡
        )
        logger.info("âœ… å…¨å±€MCPæŸ¥è¯¢ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")

    return _global_optimizer


async def close_global_optimizer():
    """å…³é—­å…¨å±€ä¼˜åŒ–å™¨"""
    global _global_optimizer
    if _global_optimizer:
        await _global_optimizer.close()
        _global_optimizer = None
        logger.info("å…¨å±€MCPæŸ¥è¯¢ä¼˜åŒ–å™¨å·²å…³é—­")
