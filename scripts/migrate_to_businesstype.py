#!/usr/bin/env python3
"""
è¿ç§»è„šæœ¬: å°†æ‰å¹³çš„æ•°æ®ç»“æ„è½¬æ¢ä¸ºä¸šåŠ¡ç±»å‹å­ç›®å½•ç»“æ„

æ—§ç»“æ„:
data/
â”œâ”€â”€ default_knowledge_base.index
â””â”€â”€ default_knowledge_base.json

æ–°ç»“æ„:
data/
â”œâ”€â”€ default/
â”‚   â”œâ”€â”€ default_knowledge_base.index
â”‚   â””â”€â”€ default_knowledge_base.json
â”œâ”€â”€ sd/
â”‚   â”œâ”€â”€ sd_knowledge_base.index
â”‚   â””â”€â”€ sd_knowledge_base.json
â””â”€â”€ warning/
    â”œâ”€â”€ warning_knowledge_base.index
    â””â”€â”€ warning_knowledge_base.json
"""

import os
import shutil
from pathlib import Path
import sys


def migrate_old_data(data_dir: str, dry_run: bool = False):
    """
    è¿ç§»æ—§çš„æ‰å¹³ç»“æ„åˆ°æ–°çš„åˆ†å±‚ç»“æ„

    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        dry_run: æ˜¯å¦åªæ˜¯é¢„è§ˆè€Œä¸å®é™…æ‰§è¡Œè¿ç§»

    Returns:
        bool: è¿ç§»æ˜¯å¦æˆåŠŸ
    """
    data_path = Path(data_dir)

    if not data_path.exists():
        print(f"âŒ é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return False

    if not data_path.is_dir():
        print(f"âŒ é”™è¯¯: è·¯å¾„ä¸æ˜¯ç›®å½•: {data_dir}")
        return False

    # æŸ¥æ‰¾æ—§æ ¼å¼çš„æ–‡ä»¶
    old_index_files = list(data_path.glob("*_knowledge_base.index"))
    old_metadata_files = list(data_path.glob("*_knowledge_base.json"))

    if not old_index_files:
        print("â„¹ï¸  æœªæ‰¾åˆ°æ—§æ ¼å¼æ–‡ä»¶ï¼Œæ— éœ€è¿ç§»ã€‚")
        return True

    print(f"ğŸ“Š æ‰¾åˆ° {len(old_index_files)} ä¸ªç´¢å¼•æ–‡ä»¶éœ€è¦è¿ç§»")
    print()

    success_count = 0
    failed_count = 0

    for old_index in old_index_files:
        try:
            # ä»æ–‡ä»¶åæå–ä¸šåŠ¡ç±»å‹ID
            # ä¾‹å¦‚: "default_knowledge_base.index" -> "default"
            business_type = old_index.stem.replace("_knowledge_base", "")

            # åˆ›å»ºæ–°çš„å­ç›®å½•
            new_dir = data_path / business_type

            if not dry_run:
                new_dir.mkdir(exist_ok=True)

            # ç§»åŠ¨ç´¢å¼•æ–‡ä»¶
            new_index = new_dir / old_index.name

            if new_index.exists():
                print(f"âŠ˜ è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {new_index}")
            else:
                if dry_run:
                    print(f"[é¢„è§ˆ] å°†è¿ç§»: {old_index} -> {new_index}")
                else:
                    shutil.move(str(old_index), str(new_index))
                    print(f"âœ“ å·²è¿ç§»: {old_index} -> {new_index}")
                    success_count += 1

            # ç§»åŠ¨å…ƒæ•°æ®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            old_metadata = old_index.with_suffix(".json")
            new_metadata = new_dir / old_metadata.name

            if old_metadata.exists():
                if new_metadata.exists():
                    print(f"âŠ˜ è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {new_metadata}")
                else:
                    if dry_run:
                        print(f"[é¢„è§ˆ] å°†è¿ç§»: {old_metadata} -> {new_metadata}")
                    else:
                        shutil.move(str(old_metadata), str(new_metadata))
                        print(f"âœ“ å·²è¿ç§»: {old_metadata} -> {new_metadata}")

        except Exception as e:
            print(f"âŒ è¿ç§»å¤±è´¥ {old_index}: {e}")
            failed_count += 1
            return False

    print()
    if failed_count == 0:
        print(f"âœ… è¿ç§»å®Œæˆ: {success_count}/{len(old_index_files)} ä¸ªæ–‡ä»¶è¿ç§»æˆåŠŸ")
        return True
    else:
        print(f"âš ï¸  è¿ç§»å®Œæˆä½†å­˜åœ¨å¤±è´¥: {success_count} æˆåŠŸ, {failed_count} å¤±è´¥")
        return False


def verify_migration(data_dir: str):
    """éªŒè¯è¿ç§»ç»“æœ"""
    data_path = Path(data_dir)

    print("\nğŸ“‹ éªŒè¯è¿ç§»ç»“æœ:")
    print("=" * 60)

    # æ£€æŸ¥å­ç›®å½•
    subdirs = [d for d in data_path.iterdir() if d.is_dir() and not d.name.startswith('.')]
    print(f"\nå­ç›®å½•æ•°é‡: {len(subdirs)}")

    for subdir in sorted(subdirs):
        index_files = list(subdir.glob("*.index"))
        metadata_files = list(subdir.glob("*.json"))

        print(f"\nğŸ“ {subdir.name}/")
        print(f"  - ç´¢å¼•æ–‡ä»¶: {len(index_files)}")
        for idx in index_files:
            print(f"    âœ“ {idx.name}")
        print(f"  - å…ƒæ•°æ®æ–‡ä»¶: {len(metadata_files)}")
        for meta in metadata_files:
            print(f"    âœ“ {meta.name}")

    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é—ç•™çš„æ—§æ ¼å¼æ–‡ä»¶
    old_index_files = list(data_path.glob("*_knowledge_base.index"))
    old_metadata_files = list(data_path.glob("*_knowledge_base.json"))

    if old_index_files or old_metadata_files:
        print(f"\nâš ï¸  è­¦å‘Š: ä»æœ‰æ—§æ ¼å¼æ–‡ä»¶æœªè¿ç§»:")
        for f in old_index_files + old_metadata_files:
            print(f"  - {f}")
    else:
        print(f"\nâœ… æ²¡æœ‰é—ç•™çš„æ—§æ ¼å¼æ–‡ä»¶")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    data_dir = "./data"
    dry_run = False

    if len(sys.argv) > 1:
        if sys.argv[1] == "--dry-run" or sys.argv[1] == "-n":
            dry_run = True
        elif len(sys.argv) > 2:
            data_dir = sys.argv[1]
            if sys.argv[2] == "--dry-run" or sys.argv[2] == "-n":
                dry_run = True
        else:
            data_dir = sys.argv[1]

    print("=" * 60)
    print("FAISS å‘é‡æ•°æ®åº“è¿ç§»è„šæœ¬")
    print("=" * 60)
    print(f"ç›®æ ‡ç›®å½•: {data_dir}")
    if dry_run:
        print("æ¨¡å¼: é¢„è§ˆï¼ˆä¸ä¼šå®é™…è¿ç§»æ–‡ä»¶ï¼‰")
    print()

    # ç¡®è®¤æ˜¯å¦ç»§ç»­
    if not dry_run:
        response = input("âš ï¸  æ­¤æ“ä½œå°†ç§»åŠ¨æ–‡ä»¶ã€‚ç»§ç»­è¿ç§»? (yes/no): ")
        if response.lower() != "yes":
            print("âŒ è¿ç§»å·²å–æ¶ˆ")
            sys.exit(0)
        print()

    # æ‰§è¡Œè¿ç§»
    success = migrate_old_data(data_dir, dry_run=dry_run)

    if success:
        if not dry_run:
            print()
            verify_migration(data_dir)

            print("\nğŸ“ åç»­æ­¥éª¤:")
            print("1. éªŒè¯æ–°çš„ç›®å½•ç»“æ„: ls -R data/")
            print("2. æµ‹è¯•åº”ç”¨: docker-compose up")
            print("3. ç¡®è®¤æ— è¯¯ååˆ é™¤å¤‡ä»½: rm -rf data.backup.*")
        sys.exit(0)
    else:
        print("\nâŒ è¿ç§»å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
        sys.exit(1)


if __name__ == "__main__":
    main()
