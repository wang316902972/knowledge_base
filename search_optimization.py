#!/usr/bin/env python3
"""
æœç´¢ç²¾åº¦ä¼˜åŒ–æ¨¡å—
æä¾›å¤šç§ç­–ç•¥æ¥æå‡FAISSå‘é‡æœç´¢çš„ç²¾åº¦
"""

import faiss
import numpy as np
import json
import logging
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from sentence_transformers import SentenceTransformer
import re
from collections import namedtuple

logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """æœç´¢ç»“æœå°è£…"""
    text: str
    score: float
    faiss_id: int
    relevance_score: float = 0.0
    diversity_rank: int = 0

@dataclass
class QualityMetrics:
    """æœç´¢è´¨é‡æŒ‡æ ‡"""
    avg_relevance_score: float
    diversity_score: float
    coverage_ratio: float
    precision_at_k: float

class SemanticChunker:
    """è¯­ä¹‰æ„ŸçŸ¥çš„æ–‡æœ¬åˆ†å—å™¨"""

    def __init__(self, model: SentenceTransformer,
                 min_chunk_size: int = 100,
                 max_chunk_size: int = 800,
                 similarity_threshold: float = 0.85):
        self.model = model
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.similarity_threshold = similarity_threshold

    def _split_sentences(self, text: str) -> List[str]:
        """å¥å­åˆ†å‰²"""
        # æ”¯æŒä¸­è‹±æ–‡çš„å¥å­åˆ†å‰²
        sentence_endings = re.compile(r'[.!?ã€‚ï¼ï¼Ÿ]+[\s\n]+')
        sentences = sentence_endings.split(text.strip())
        return [s.strip() for s in sentences if s.strip()]

    def _semantic_chunk(self, sentences: List[str]) -> List[str]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„åˆ†å—"""
        if len(sentences) <= 1:
            return sentences

        chunks = []
        current_chunk = []
        current_length = 0

        # è·å–å¥å­å‘é‡
        sentence_vectors = self.model.encode(
            sentences,
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        for i, sentence in enumerate(sentences):
            sentence_length = len(sentence)

            # å¦‚æœå½“å‰å—ä¸ºç©ºï¼Œç›´æ¥æ·»åŠ 
            if not current_chunk:
                current_chunk.append(sentence)
                current_length = sentence_length
                continue

            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + sentence_length > self.max_chunk_size:
                if len(current_chunk) >= self.min_chunk_size:
                    chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_length = sentence_length
                continue

            # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
            if i > 0:
                similarity = np.dot(sentence_vectors[i], sentence_vectors[i-1])

                # å¦‚æœç›¸ä¼¼åº¦è¿‡ä½ï¼Œå¼€å§‹æ–°çš„åˆ†å—
                if similarity < self.similarity_threshold and len(current_chunk) >= self.min_chunk_size:
                    chunks.append(' '.join(current_chunk))
                    current_chunk = [sentence]
                    current_length = sentence_length
                    continue

            current_chunk.append(sentence)
            current_length += sentence_length

        # æ·»åŠ æœ€åä¸€ä¸ªåˆ†å—
        if current_chunk and len(current_chunk) >= self.min_chunk_size:
            chunks.append(' '.join(current_chunk))

        return chunks

    def chunk_text(self, text: str) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†å—"""
        sentences = self._split_sentences(text)

        if len(sentences) <= 3:  # çŸ­æ–‡æœ¬ä¸è¿›è¡Œåˆ†å—
            return [text] if len(text) >= self.min_chunk_size else []

        return self._semantic_chunk(sentences)

class SearchQualityOptimizer:
    """æœç´¢è´¨é‡ä¼˜åŒ–å™¨"""

    def __init__(self, model: SentenceTransformer):
        self.model = model

    def calculate_relevance_scores(self, query: str, results: List[SearchResult]) -> List[float]:
        """è®¡ç®—ç›¸å…³æ€§å¾—åˆ†"""
        if not results:
            return []

        # è·å–æŸ¥è¯¢å‘é‡
        query_vector = self.model.encode(
            [query],
            convert_to_numpy=True,
            normalize_embeddings=True
        )[0]

        # è·å–ç»“æœæ–‡æœ¬å‘é‡
        result_texts = [result.text for result in results]
        result_vectors = self.model.encode(
            result_texts,
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        relevance_scores = []
        for i, result_vector in enumerate(result_vectors):
            similarity = np.dot(query_vector, result_vector)
            relevance_scores.append(similarity)

        return relevance_scores

    def calculate_diversity_score(self, results: List[SearchResult]) -> float:
        """è®¡ç®—ç»“æœå¤šæ ·æ€§å¾—åˆ†"""
        if len(results) <= 1:
            return 0.0

        texts = [result.text for result in results]
        vectors = self.model.encode(
            texts,
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        # è®¡ç®—ä¸¤ä¸¤ä¹‹é—´çš„å¹³å‡è·ç¦»
        total_distance = 0
        count = 0

        for i in range(len(vectors)):
            for j in range(i + 1, len(vectors)):
                distance = 1 - np.dot(vectors[i], vectors[j])  # è½¬æ¢ä¸ºè·ç¦»
                total_distance += distance
                count += 1

        return total_distance / count if count > 0 else 0.0

    def rerank_by_diversity(self, results: List[SearchResult],
                          diversity_weight: float = 0.3) -> List[SearchResult]:
        """åŸºäºå¤šæ ·æ€§é‡æ–°æ’åº"""
        if len(results) <= 1:
            return results

        # è®¡ç®—æ¯å¯¹ç»“æœä¹‹é—´çš„è·ç¦»
        texts = [result.text for result in results]
        vectors = self.model.encode(
            texts,
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        # å»ºç«‹faiss_idåˆ°ç»“æœç´¢å¼•çš„æ˜ å°„
        faiss_id_to_result_idx = {result.faiss_id: i for i, result in enumerate(results)}

        # ä½¿ç”¨æœ€å¤§è¾¹é™…ç›¸å…³æ€§(MMR)ç®—æ³•
        reranked = []
        remaining_indices = list(range(len(results)))

        # é€‰æ‹©æœ€å¥½çš„ç¬¬ä¸€ä¸ªç»“æœ
        first_idx = np.argmax([result.score for result in results])
        reranked.append(results[first_idx])
        remaining_indices.remove(first_idx)

        while remaining_indices:
            best_score = float('-inf')
            best_idx = -1

            for idx in remaining_indices:
                # åŸå§‹ç›¸å…³æ€§å¾—åˆ†
                relevance = results[idx].score

                # ä¸å·²é€‰æ‹©ç»“æœçš„æœ€å¤§ç›¸ä¼¼åº¦
                max_similarity = 0
                for selected in reranked:
                    # ä½¿ç”¨æ­£ç¡®çš„ç»“æœç´¢å¼•ï¼Œè€Œä¸æ˜¯faiss_id
                    selected_idx = faiss_id_to_result_idx[selected.faiss_id]
                    similarity = np.dot(vectors[idx], vectors[selected_idx])
                    max_similarity = max(max_similarity, similarity)

                # MMRå¾—åˆ†
                mmr_score = (1 - diversity_weight) * relevance - diversity_weight * max_similarity

                if mmr_score > best_score:
                    best_score = mmr_score
                    best_idx = idx

            reranked.append(results[best_idx])
            remaining_indices.remove(best_idx)

        return reranked

class AdvancedSearchIndex:
    """é«˜çº§æœç´¢ç´¢å¼•ç±»"""

    def __init__(self, config):
        self.config = config
        self.embedding_model = SentenceTransformer(config.MODEL_NAME)
        self.semantic_chunker = SemanticChunker(
            self.embedding_model,
            min_chunk_size=config.MIN_CHUNK_SIZE,
            max_chunk_size=config.MAX_CHUNK_SIZE
        )
        self.quality_optimizer = SearchQualityOptimizer(self.embedding_model)
        self.index = None
        self.id_to_chunk = {}
        self.chunk_to_id = {}

        # æœç´¢é…ç½®
        self.search_config = {
            'ivf_nprobe': min(config.NPROBE, config.NLIST // 2),  # åŠ¨æ€è°ƒæ•´nprobe
            'hnsw_ef_search': max(config.EF_SEARCH, 100),  # å¢åŠ æœç´¢ç²¾åº¦
            'diversity_weight': 0.1,  # å¤šæ ·æ€§æƒé‡ï¼ˆé™ä½ä»¥ä¿æŒç›¸å…³æ€§ä¼˜å…ˆï¼‰
            'relevance_threshold': 0.1  # ç›¸å…³æ€§é˜ˆå€¼ï¼ˆé™ä½ä»¥é¿å…è¿‡åº¦è¿‡æ»¤ï¼‰
        }

    def _create_optimized_index(self):
        """åˆ›å»ºä¼˜åŒ–çš„ç´¢å¼•"""
        if self.config.INDEX_TYPE == "IVFFlat":
            # ä¼˜åŒ–çš„IVFç´¢å¼•
            nlist = self.config.NLIST
            quantizer = faiss.IndexFlatIP(self.config.EMBEDDING_DIM)
            index = faiss.IndexIVFFlat(quantizer, self.config.EMBEDDING_DIM, nlist)

            # è®¾ç½®ä¼˜åŒ–çš„è®­ç»ƒå‚æ•°
            index.nlist = nlist
            return index

        elif self.config.INDEX_TYPE == "HNSW":
            # ä¼˜åŒ–çš„HNSWç´¢å¼•
            M = self.config.M
            ef_construction = self.config.EF_CONSTRUCTION
            index = faiss.IndexHNSWFlat(self.config.EMBEDDING_DIM, M)
            index.hnsw.efConstruction = ef_construction
            index.hnsw.efSearch = self.search_config['hnsw_ef_search']

            return index

        else:
            # å›é€€åˆ°ç²¾ç¡®æœç´¢
            return faiss.IndexFlatIP(self.config.EMBEDDING_DIM)

    def optimized_search(self, query: str, top_k: int = 10,
                        use_reranking: bool = True) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–çš„æœç´¢æ–¹æ³•"""
        if not self.index or self.index.ntotal == 0:
            return []

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_vector = self.embedding_model.encode(
            [query],
            convert_to_numpy=True,
            normalize_embeddings=True
        )

        # åŠ¨æ€è°ƒæ•´æœç´¢å‚æ•°
        search_k = min(top_k * 3, self.index.ntotal)  # æœç´¢æ›´å¤šå€™é€‰

        # è®¾ç½®ç´¢å¼•ç‰¹å®šçš„æœç´¢å‚æ•°
        if hasattr(self.index, 'nprobe'):  # IVFç´¢å¼•
            self.index.nprobe = self.search_config['ivf_nprobe']
        elif hasattr(self.index, 'hnsw'):  # HNSWç´¢å¼•
            self.index.hnsw.efSearch = self.search_config['hnsw_ef_search']

        # æ‰§è¡Œæœç´¢
        distances, indices = self.index.search(query_vector, search_k)

        # æ„å»ºç»“æœå¯¹è±¡
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx != -1 and str(idx) in self.id_to_chunk:
                result = SearchResult(
                    text=self.id_to_chunk[str(idx)],
                    score=float(dist),
                    faiss_id=int(idx)
                )
                results.append(result)

        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        relevance_scores = self.quality_optimizer.calculate_relevance_scores(query, results)
        for i, score in enumerate(relevance_scores):
            results[i].relevance_score = score

        # è‡ªé€‚åº”é˜ˆå€¼ï¼šå¦‚æœé«˜è´¨é‡ç»“æœä¸å¤Ÿï¼Œæ”¾å®½é˜ˆå€¼
        threshold = self.search_config['relevance_threshold']
        filtered_results = [r for r in results if r.relevance_score >= threshold]

        # å¦‚æœè¿‡æ»¤åç»“æœå¤ªå°‘ï¼Œæ”¾å®½é˜ˆå€¼
        if len(filtered_results) < top_k:
            # ä½¿ç”¨æ›´ä½çš„é˜ˆå€¼
            relaxed_threshold = max(0.05, threshold * 0.5)
            filtered_results = [r for r in results if r.relevance_score >= relaxed_threshold]

        # å¦‚æœç»“æœä»ç„¶å¤ªå°‘ï¼Œä½¿ç”¨æ‰€æœ‰ç»“æœæŒ‰ç›¸å…³æ€§æ’åº
        if len(filtered_results) < top_k:
            # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åºæ‰€æœ‰ç»“æœ
            all_results_sorted = sorted(results, key=lambda x: x.relevance_score, reverse=True)
            filtered_results = all_results_sorted[:max(top_k, len(all_results_sorted))]

        # é‡æ–°æ’åºï¼ˆä»…åœ¨ç»“æœå……è¶³æ—¶è¿›è¡Œå¤šæ ·æ€§é‡æ’ï¼‰
        if use_reranking and len(filtered_results) > top_k // 2:
            filtered_results = self.quality_optimizer.rerank_by_diversity(
                filtered_results,
                self.search_config['diversity_weight']
            )

        # è¿”å›å‰top_kä¸ªç»“æœ
        final_results = filtered_results[:top_k]

        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        quality_metrics = self._calculate_quality_metrics(query, final_results)

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        return [
            {
                "text": result.text,
                "similarity_score": float(result.score),
                "relevance_score": float(result.relevance_score),
                "faiss_id": int(result.faiss_id),
                "quality_metrics": quality_metrics.__dict__ if i == 0 else None
            }
            for i, result in enumerate(final_results)
        ]

    def _calculate_quality_metrics(self, query: str, results: List[SearchResult]) -> QualityMetrics:
        """è®¡ç®—æœç´¢è´¨é‡æŒ‡æ ‡"""
        if not results:
            return QualityMetrics(0.0, 0.0, 0.0, 0.0)

        # å¹³å‡ç›¸å…³æ€§å¾—åˆ†
        relevance_scores = [r.relevance_score for r in results]
        avg_relevance = np.mean(relevance_scores)

        # å¤šæ ·æ€§å¾—åˆ†
        diversity_score = self.quality_optimizer.calculate_diversity_score(results)

        # è¦†ç›–ç‡ï¼ˆé«˜åˆ†ç»“æœæ¯”ä¾‹ï¼‰
        high_score_count = sum(1 for r in results if r.relevance_score > 0.7)
        coverage_ratio = high_score_count / len(results)

        # Precision@kï¼ˆå‡è®¾é˜ˆå€¼ä¸º0.6ï¼‰
        precision_at_k = sum(1 for r in results if r.relevance_score > 0.6) / len(results)

        return QualityMetrics(
            avg_relevance_score=float(avg_relevance),
            diversity_score=float(diversity_score),
            coverage_ratio=float(coverage_ratio),
            precision_at_k=float(precision_at_k)
        )

    def get_search_recommendations(self) -> Dict[str, Any]:
        """è·å–æœç´¢ä¼˜åŒ–å»ºè®®"""
        total_vectors = self.index.ntotal if self.index else 0

        recommendations = {
            "current_config": self.search_config.copy(),
            "recommendations": []
        }

        # åŸºäºæ•°æ®é‡çš„å»ºè®®
        if total_vectors < 1000:
            recommendations["recommendations"].append(
                "æ•°æ®é‡è¾ƒå°ï¼Œå»ºè®®ä½¿ç”¨FlatIPç´¢å¼•è·å¾—æœ€ä½³ç²¾åº¦"
            )
            recommendations["suggested_index_type"] = "FlatIP"
        elif total_vectors < 100000:
            recommendations["recommendations"].append(
                "æ•°æ®é‡ä¸­ç­‰ï¼Œå»ºè®®ä½¿ç”¨HNSWç´¢å¼•å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½"
            )
            recommendations["suggested_index_type"] = "HNSW"
            recommendations["suggested_hnsw_params"] = {"M": 32, "efConstruction": 200}
        else:
            recommendations["recommendations"].append(
                "æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨IVFFlatç´¢å¼•"
            )
            recommendations["suggested_index_type"] = "IVFFlat"
            recommendations["suggested_ivf_params"] = {"nlist": min(1000, total_vectors // 10)}

        # æœç´¢å‚æ•°å»ºè®®
        if hasattr(self.index, 'nprobe'):
            optimal_nprobe = min(self.config.NLIST // 4, 20)
            recommendations["recommendations"].append(
                f"å»ºè®®å°†IVF nprobeè®¾ç½®ä¸º {optimal_nprobe} ä»¥æå‡æœç´¢ç²¾åº¦"
            )

        if hasattr(self.index, 'hnsw'):
            recommendations["recommendations"].append(
                "å»ºè®®å°†HNSW efSearchè®¾ç½®ä¸º100-200ä»¥æå‡æœç´¢ç²¾åº¦"
            )

        return recommendations

# ä½¿ç”¨ç¤ºä¾‹å’Œé…ç½®
class OptimizedConfig:
    """ä¼˜åŒ–åçš„é…ç½®ç¤ºä¾‹"""
    INDEX_TYPE = "HNSW"  # æ›´å¥½çš„ç²¾åº¦æ€§èƒ½å¹³è¡¡
    MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"
    EMBEDDING_DIM = 384
    NLIST = 100
    NPROBE = 20  # å¢åŠ æ¢æµ‹æ•°é‡
    M = 32  # å¢åŠ è¿æ¥æ•°
    EF_CONSTRUCTION = 200
    EF_SEARCH = 100  # å¢åŠ æœç´¢æ·±åº¦
    MIN_CHUNK_SIZE = 100
    MAX_CHUNK_SIZE = 800

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    config = OptimizedConfig()
    optimizer = AdvancedSearchIndex(config)

    print("âœ… æœç´¢ç²¾åº¦ä¼˜åŒ–æ¨¡å—å·²åŠ è½½")
    print("ğŸ“Š ä¸»è¦ä¼˜åŒ–ç‰¹æ€§:")
    print("  - è¯­ä¹‰æ„ŸçŸ¥æ–‡æœ¬åˆ†å—")
    print("  - åŠ¨æ€æœç´¢å‚æ•°è°ƒæ•´")
    print("  - å¤šæ ·æ€§é‡æ’åº(MMR)")
    print("  - æœç´¢è´¨é‡è¯„ä¼°")
    print("  - æ™ºèƒ½ç´¢å¼•é€‰æ‹©å»ºè®®")