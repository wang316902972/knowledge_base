#!/usr/bin/env python3
"""
æœç´¢ä¼˜åŒ–ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•è„šæœ¬
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ä¼˜åŒ–åŠŸèƒ½æå‡æœç´¢ç²¾åº¦
"""

import requests
import json
import time
from typing import List, Dict

class SearchOptimizationDemo:
    """æœç´¢ä¼˜åŒ–æ¼”ç¤ºç±»"""

    def __init__(self, base_url: str = "http://localhost:8001"):
        self.base_url = base_url
        self.sample_docs = [
            """æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿ç”¨ç®—æ³•æ¥åˆ†ææ•°æ®ï¼Œä»ä¸­å­¦ä¹ æ¨¡å¼ï¼Œå¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³æ˜¯è®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ ï¼Œè€Œä¸éœ€è¦æ˜ç¡®ç¼–ç¨‹ã€‚ä¸»è¦ç±»å‹åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œå¸¸è§çš„ç®—æ³•æœ‰çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€å†³ç­–æ ‘å’Œéšæœºæ£®æ—ç­‰ã€‚æ— ç›‘ç£å­¦ä¹ åˆ™ä»æœªæ ‡è®°çš„æ•°æ®ä¸­å‘ç°éšè—çš„æ¨¡å¼å’Œç»“æ„ï¼ŒåŒ…æ‹¬èšç±»åˆ†æå’Œä¸»æˆåˆ†åˆ†æç­‰æŠ€æœ¯ã€‚æœºå™¨å­¦ä¹ åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ï¼Œä»åƒåœ¾é‚®ä»¶è¿‡æ»¤åˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œä»åŒ»ç–—è¯Šæ–­åˆ°é‡‘èé£é™©è¯„ä¼°ï¼Œéƒ½å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚""",

            """æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚æ·±åº¦å­¦ä¹ çš„"æ·±åº¦"æŒ‡çš„æ˜¯ç¥ç»ç½‘ç»œå…·æœ‰å¤šä¸ªéšè—å±‚ï¼Œè¿™äº›å±‚èƒ½å¤Ÿé€çº§æå–æ•°æ®çš„æŠ½è±¡ç‰¹å¾ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨å›¾åƒè¯†åˆ«å’Œå¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ å›¾åƒçš„ç©ºé—´å±‚æ¬¡ç»“æ„ã€‚å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰åŠå…¶å˜ä½“å¦‚é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ç‰¹åˆ«é€‚åˆå¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚è‡ªç„¶è¯­è¨€å’Œæ—¶é—´åºåˆ—ã€‚Transformeræ¶æ„çš„å‡ºç°å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå…¶è‡ªæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚""",

            """è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processing, NLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºè®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚NLPçš„ç›®æ ‡æ˜¯è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šã€ç”Ÿæˆäººç±»è¯­è¨€ã€‚ä¸»è¦ä»»åŠ¡åŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–ã€æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿå’Œæ–‡æœ¬æ‘˜è¦ç­‰ã€‚ä¼ ç»Ÿçš„NLPæ–¹æ³•ä¾èµ–äºè¯­è¨€å­¦è§„åˆ™å’Œç»Ÿè®¡æ¨¡å‹ï¼Œè€Œç°ä»£NLPä¸»è¦åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚è¯åµŒå…¥æŠ€æœ¯å¦‚Word2Vecå’ŒGloVeå°†è¯è¯­æ˜ å°„åˆ°å‘é‡ç©ºé—´ï¼Œæ•æ‰è¯­ä¹‰å…³ç³»ã€‚é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¦‚BERTã€GPTå’ŒT5åœ¨å„ç§NLPä»»åŠ¡ä¸Šéƒ½å–å¾—äº†state-of-the-artçš„æ€§èƒ½ã€‚NLPæŠ€æœ¯å¹¿æ³›åº”ç”¨äºæœç´¢å¼•æ“ã€æ™ºèƒ½å®¢æœã€å†…å®¹æ¨èã€æœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸã€‚""",

            """è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œè§£é‡Šè§†è§‰ä¿¡æ¯ã€‚è®¡ç®—æœºè§†è§‰çš„ç›®æ ‡æ˜¯è®©æœºå™¨èƒ½å¤Ÿåƒäººç±»ä¸€æ ·"çœ‹æ‡‚"å›¾åƒå’Œè§†é¢‘ã€‚æ ¸å¿ƒä»»åŠ¡åŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²ã€äººè„¸è¯†åˆ«ã€å§¿æ€ä¼°è®¡å’Œå›¾åƒç”Ÿæˆç­‰ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºæ‰‹å·¥è®¾è®¡çš„ç‰¹å¾æå–å™¨ï¼Œè€Œç°ä»£è®¡ç®—æœºè§†è§‰ä¸»è¦åŸºäºæ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚CNNèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ å›¾åƒçš„å¤šå±‚æ¬¡ç‰¹å¾è¡¨ç¤ºï¼Œä»è¾¹ç¼˜å’Œçº¹ç†åˆ°å¤æ‚çš„å½¢çŠ¶å’Œå¯¹è±¡ã€‚é¢„è®­ç»ƒæ¨¡å‹å¦‚ResNetã€EfficientNetå’ŒVision Transformeråœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚è®¡ç®—æœºè§†è§‰æŠ€æœ¯å¹¿æ³›åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€åŒ»å­¦å½±åƒåˆ†æã€å®‰é˜²ç›‘æ§ã€å·¥ä¸šæ£€æµ‹ç­‰é¢†åŸŸã€‚""",

            """å¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œæ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ™ºèƒ½ä½“é‡‡å–è¡ŒåŠ¨è·å¾—å¥–åŠ±æˆ–æƒ©ç½šï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚å…³é”®æ¦‚å¿µåŒ…æ‹¬çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€ç­–ç•¥å’Œä»·å€¼å‡½æ•°ã€‚é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›äº†æ•°å­¦æ¡†æ¶ã€‚ä¸»è¦ç®—æ³•åŒ…æ‹¬Q-Learningã€æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰ã€ç­–ç•¥æ¢¯åº¦æ–¹æ³•å’Œæ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ã€‚å¼ºåŒ–å­¦ä¹ åœ¨æ¸¸æˆAIï¼ˆå¦‚AlphaGoï¼‰ã€æœºå™¨äººæ§åˆ¶ã€èµ„æºè°ƒåº¦ã€é‡‘èæŠ•èµ„ç»„åˆç®¡ç†ç­‰é¢†åŸŸæœ‰é‡è¦åº”ç”¨ã€‚ç„¶è€Œï¼Œå¼ºåŒ–å­¦ä¹ ä¹Ÿé¢ä¸´ç€æ ·æœ¬æ•ˆç‡ä½ã€æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ã€å¥–åŠ±è®¾è®¡å›°éš¾ç­‰æŒ‘æˆ˜ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ç»“åˆäº†æ·±åº¦å­¦ä¹ çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚""",

            """æ•°æ®æŒ–æ˜æ˜¯ä»å¤§é‡æ•°æ®ä¸­å‘ç°æœ‰ç”¨æ¨¡å¼å’ŒçŸ¥è¯†çš„è¿‡ç¨‹ã€‚æ•°æ®æŒ–æ˜ç»“åˆäº†ç»Ÿè®¡å­¦ã€æœºå™¨å­¦ä¹ ã€æ•°æ®åº“ç³»ç»Ÿå’Œå¯è§†åŒ–æŠ€æœ¯ï¼Œæ—¨åœ¨ä»æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„æ¨¡å¼ã€‚ä¸»è¦ä»»åŠ¡åŒ…æ‹¬åˆ†ç±»ã€å›å½’ã€èšç±»ã€å…³è”è§„åˆ™æŒ–æ˜ã€å¼‚å¸¸æ£€æµ‹å’Œæ—¶é—´åºåˆ—åˆ†æç­‰ã€‚åˆ†ç±»ä»»åŠ¡é¢„æµ‹ç¦»æ•£çš„ç›®æ ‡å˜é‡ï¼Œå¦‚åƒåœ¾é‚®ä»¶æ£€æµ‹ã€‚å›å½’ä»»åŠ¡é¢„æµ‹è¿ç»­å€¼ï¼Œå¦‚æˆ¿ä»·é¢„æµ‹ã€‚èšç±»å°†ç›¸ä¼¼çš„æ•°æ®ç‚¹åˆ†ç»„ï¼Œå¦‚å®¢æˆ·ç»†åˆ†ã€‚å…³è”è§„åˆ™å‘ç°é¡¹ä¹‹é—´çš„å…³ç³»ï¼Œå¦‚è´­ç‰©ç¯®åˆ†æã€‚å¼‚å¸¸æ£€æµ‹è¯†åˆ«ä¸æ­£å¸¸æ¨¡å¼ä¸åŒçš„æ•°æ®ç‚¹ï¼Œå¦‚æ¬ºè¯ˆæ£€æµ‹ã€‚æ•°æ®æŒ–æ˜è¿‡ç¨‹åŒ…æ‹¬æ•°æ®æ¸…æ´—ã€æ•°æ®é›†æˆã€æ•°æ®é€‰æ‹©ã€æ•°æ®è½¬æ¢ã€æ¨¡å¼æŒ–æ˜å’Œæ¨¡å¼è¯„ä¼°ç­‰æ­¥éª¤ã€‚å¸¸ç”¨çš„å·¥å…·æœ‰Pythonçš„scikit-learnã€pandasï¼ŒRè¯­è¨€ï¼Œä»¥åŠä¸“é—¨çš„è½¯ä»¶å¦‚Wekaå’ŒKNIMEã€‚""",

            """Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´çš„è¯­æ³•ã€å¼ºå¤§çš„åŠŸèƒ½å’Œä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿè€Œé—»åã€‚Pythonçš„è®¾è®¡å“²å­¦å¼ºè°ƒä»£ç çš„å¯è¯»æ€§å’Œç®€æ´æ€§ï¼Œä½¿ç”¨ç¼©è¿›æ¥å®šä¹‰ä»£ç å—ã€‚Pythonæ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ï¼ŒåŒ…æ‹¬é¢å‘å¯¹è±¡ç¼–ç¨‹ã€å‡½æ•°å¼ç¼–ç¨‹å’Œè¿‡ç¨‹å¼ç¼–ç¨‹ã€‚Pythonæ‹¥æœ‰å¼ºå¤§çš„æ ‡å‡†åº“ï¼Œæ¶µç›–äº†æ–‡ä»¶æ“ä½œã€ç½‘ç»œé€šä¿¡ã€æ•°æ®å¤„ç†ã€å›¾å½¢ç•Œé¢ç­‰å„ä¸ªæ–¹é¢ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒPythonæ‹¥æœ‰åºå¤§çš„ç¬¬ä¸‰æ–¹åŒ…ç”Ÿæ€ç³»ç»Ÿï¼ŒPyPIï¼ˆPython Package Indexï¼‰ä¸Šæœ‰è¶…è¿‡300,000ä¸ªåŒ…ã€‚åœ¨æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸï¼ŒPythonæ˜¯äº‹å®ä¸Šçš„æ ‡å‡†è¯­è¨€ï¼Œæ‹¥æœ‰NumPyã€Pandasã€Matplotlibã€Scikit-learnã€TensorFlowã€PyTorchç­‰å¼ºå¤§çš„åº“ã€‚Pythonåœ¨Webå¼€å‘ã€è‡ªåŠ¨åŒ–è„šæœ¬ã€ç§‘å­¦è®¡ç®—ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚""",

            """TensorFlowæ˜¯Googleå¼€å‘çš„å¼€æºæœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºæ·±åº¦å­¦ä¹ å’Œå¤§è§„æ¨¡æ•°å€¼è®¡ç®—ã€‚TensorFlowæä¾›äº†çµæ´»çš„ç¼–ç¨‹æ¨¡å‹ï¼Œå¯ä»¥éƒ¨ç½²åˆ°å„ç§å¹³å°ï¼Œä»ç§»åŠ¨è®¾å¤‡åˆ°åˆ†å¸ƒå¼è®¡ç®—é›†ç¾¤ã€‚æ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬å¼ é‡ï¼ˆå¤šç»´æ•°ç»„ï¼‰ã€è®¡ç®—å›¾ï¼ˆè¡¨ç¤ºè®¡ç®—çš„å›¾å½¢ç»“æ„ï¼‰ã€ä¼šè¯ï¼ˆæ‰§è¡Œè®¡ç®—å›¾çš„ç¯å¢ƒï¼‰å’Œå˜é‡ï¼ˆå­˜å‚¨æ¨¡å‹å‚æ•°ï¼‰ã€‚TensorFlow 2.xå¼•å…¥äº†Eager Executionï¼Œä½¿å¼€å‘æ›´åŠ ç›´è§‚ï¼ŒåŒæ—¶ä¿æŒäº†TensorFlow 1.xçš„æ€§èƒ½ä¼˜åŠ¿ã€‚Kerasä½œä¸ºTensorFlowçš„é«˜çº§APIï¼Œæä¾›äº†ç®€æ´çš„æ¥å£æ¥æ„å»ºå’Œè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚TensorFlowæ‹¥æœ‰ä¸°å¯Œçš„é¢„è®­ç»ƒæ¨¡å‹åº“ï¼ˆTensorFlow Hubï¼‰å’Œéƒ¨ç½²å·¥å…·ï¼ˆTensorFlow Liteã€TensorFlow.jsã€TensorFlow Servingï¼‰ã€‚TensorFlowåœ¨å·¥ä¸šç•Œå’Œå­¦æœ¯ç•Œéƒ½æœ‰å¹¿æ³›åº”ç”¨ï¼Œæ”¯æŒä»ç ”ç©¶åŸå‹åˆ°ç”Ÿäº§éƒ¨ç½²çš„å®Œæ•´æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸã€‚""",

            """PyTorchæ˜¯Facebookï¼ˆç°Metaï¼‰äººå·¥æ™ºèƒ½ç ”ç©¶å›¢é˜Ÿå¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä»¥å…¶çµæ´»æ€§å’Œæ˜“ç”¨æ€§è€Œå—åˆ°ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…çš„é’çã€‚PyTorchçš„æ ¸å¿ƒç‰¹ç‚¹æ˜¯åŠ¨æ€è®¡ç®—å›¾ï¼ˆDefine-by-Runï¼‰ï¼Œå…è®¸åœ¨è¿è¡Œæ—¶ä¿®æ”¹è®¡ç®—å›¾ï¼Œè¿™ä½¿å¾—è°ƒè¯•æ›´åŠ ç›´è§‚ï¼Œä¾¿äºå®ç°å¤æ‚çš„æ¨¡å‹ç»“æ„ã€‚PyTorchæä¾›äº†å¼ºå¤§çš„å¼ é‡æ“ä½œåº“ï¼Œæ”¯æŒGPUåŠ é€Ÿè®¡ç®—ï¼Œå¹¶å…·æœ‰è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½ï¼Œç®€åŒ–äº†æ¢¯åº¦è®¡ç®—è¿‡ç¨‹ã€‚torch.nnæ¨¡å—æä¾›äº†æ„å»ºç¥ç»ç½‘ç»œæ‰€éœ€çš„ç»„ä»¶ï¼Œå¦‚å±‚ã€æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ã€‚PyTorchç”Ÿæ€ç³»ç»ŸåŒ…æ‹¬torchvisionï¼ˆè®¡ç®—æœºè§†è§‰ï¼‰ã€torchaudioï¼ˆéŸ³é¢‘å¤„ç†ï¼‰ã€torchtextï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰ç­‰ä¸“é—¨åº“ã€‚PyTorchåœ¨å­¦æœ¯ç•Œç‰¹åˆ«å—æ¬¢è¿ï¼Œæ˜¯è®¸å¤šç ”ç©¶è®ºæ–‡çš„é¦–é€‰æ¡†æ¶ã€‚è¿‘å¹´æ¥ï¼ŒPyTorchåœ¨å·¥ä¸šç•Œçš„åº”ç”¨ä¹Ÿåœ¨å¿«é€Ÿå¢é•¿ï¼Œè®¸å¤šå…¬å¸é€‰æ‹©PyTorchä½œä¸ºä¸»è¦çš„æ·±åº¦å­¦ä¹ å¹³å°ã€‚""",

            """ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œç”±å¤šä¸ªç›¸äº’è¿æ¥çš„ç¥ç»å…ƒå±‚ç»„æˆã€‚æ¯ä¸ªç¥ç»å…ƒæ¥æ”¶è¾“å…¥ä¿¡å·ï¼Œåº”ç”¨æ¿€æ´»å‡½æ•°ï¼Œå¹¶äº§ç”Ÿè¾“å‡ºä¿¡å·ä¼ é€’ç»™ä¸‹ä¸€å±‚çš„ç¥ç»å…ƒã€‚åŸºæœ¬çš„ç¥ç»ç½‘ç»œç»“æ„åŒ…æ‹¬è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ã€‚å‰é¦ˆç¥ç»ç½‘ç»œæ˜¯æœ€ç®€å•çš„ç±»å‹ï¼Œä¿¡æ¯å•å‘æµåŠ¨ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸“é—¨ç”¨äºå¤„ç†ç½‘æ ¼çŠ¶æ•°æ®ï¼Œå¦‚å›¾åƒï¼Œä½¿ç”¨å·ç§¯å±‚è‡ªåŠ¨å­¦ä¹ ç©ºé—´ç‰¹å¾ã€‚å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰é€‚åˆå¤„ç†åºåˆ—æ•°æ®ï¼Œå…·æœ‰è®°å¿†åŠŸèƒ½æ¥æ•æ‰æ—¶é—´ä¾èµ–å…³ç³»ã€‚é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰å’Œé—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰è§£å†³äº†ä¼ ç»ŸRNNçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚Transformeræ¶æ„åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå¹¶è¡Œå¤„ç†åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç¥ç»ç½‘ç»œçš„è®­ç»ƒé€šå¸¸ä½¿ç”¨åå‘ä¼ æ’­ç®—æ³•å’Œéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰åŠå…¶å˜ç§æ¥ä¼˜åŒ–ç½‘ç»œå‚æ•°ã€‚"""
        ]

        self.test_queries = [
            "æœºå™¨å­¦ä¹ çš„ä¸»è¦ç±»å‹æœ‰å“ªäº›ï¼Ÿ",
            "å·ç§¯ç¥ç»ç½‘ç»œå’Œå¾ªç¯ç¥ç»ç½‘ç»œçš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
            "Pythonåœ¨æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ä¸­çš„ä¼˜åŠ¿",
            "Transformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä½œç”¨",
            "å¼ºåŒ–å­¦ä¹ åœ¨æ¸¸æˆAIä¸­çš„åº”ç”¨æ¡ˆä¾‹",
            "æ•°æ®æŒ–æ˜çš„ä¸»è¦ä»»åŠ¡å’ŒæŠ€æœ¯",
            "TensorFlowå’ŒPyTorchçš„ç‰¹ç‚¹å¯¹æ¯”",
            "è®¡ç®—æœºè§†è§‰çš„æ ¸å¿ƒä»»åŠ¡æœ‰å“ªäº›ï¼Ÿ",
            "ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­çš„åå‘ä¼ æ’­ç®—æ³•",
            "æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨"
        ]

    def add_sample_documents(self):
        """æ·»åŠ ç¤ºä¾‹æ–‡æ¡£"""
        print("ğŸ“š æ­£åœ¨æ·»åŠ ç¤ºä¾‹æ–‡æ¡£...")

        for i, doc in enumerate(self.sample_docs):
            response = requests.post(
                f"{self.base_url}/add",
                json={
                    "content": doc,
                    "chunk_size": 500,
                    "chunk_overlap": 100
                }
            )

            if response.status_code == 200:
                print(f"âœ… æ–‡æ¡£ {i+1}/10 æ·»åŠ æˆåŠŸ")
            else:
                print(f"âŒ æ–‡æ¡£ {i+1} æ·»åŠ å¤±è´¥: {response.text}")

        # æ‰‹åŠ¨ä¿å­˜
        save_response = requests.post(f"{self.base_url}/save")
        if save_response.status_code == 200:
            print("ğŸ’¾ æ–‡æ¡£å·²ä¿å­˜")

        print()

    def compare_search_methods(self):
        """å¯¹æ¯”ä¼ ç»Ÿæœç´¢å’Œä¼˜åŒ–æœç´¢"""
        print("ğŸ” æœç´¢ç²¾åº¦å¯¹æ¯”æµ‹è¯•")
        print("=" * 50)

        for query in self.test_queries:
            print(f"\nğŸ“ æŸ¥è¯¢: {query}")

            # ä¼ ç»Ÿæœç´¢
            traditional_response = requests.post(
                f"{self.base_url}/search",
                json={
                    "question": query,
                    "top_k": 5,
                    "use_optimization": False
                }
            )

            # ä¼˜åŒ–æœç´¢
            optimized_response = requests.post(
                f"{self.base_url}/search",
                json={
                    "question": query,
                    "top_k": 5,
                    "use_optimization": True
                }
            )

            if traditional_response.status_code == 200:
                trad_data = traditional_response.json()
                trad_scores = [r.get('score', 0) for r in trad_data['detailed_results']]
                avg_trad = sum(trad_scores) / len(trad_scores) if trad_scores else 0
                print(f"  ğŸ“Š ä¼ ç»Ÿæœç´¢å¹³å‡å¾—åˆ†: {avg_trad:.4f}")

                if optimized_response.status_code == 200:
                    opt_data = optimized_response.json()
                    opt_scores = [r.get('relevance_score', r.get('score', 0)) for r in opt_data['detailed_results']]
                    avg_opt = sum(opt_scores) / len(opt_scores) if opt_scores else 0
                    improvement = ((avg_opt - avg_trad) / avg_trad * 100) if avg_trad > 0 else 0

                    print(f"  ğŸ“ˆ ä¼˜åŒ–æœç´¢å¹³å‡å¾—åˆ†: {avg_opt:.4f}")
                    print(f"  ğŸš€ ç²¾åº¦æå‡: {improvement:+.2f}%")

                    # æ˜¾ç¤ºæœ€ä½³ç»“æœ
                    if opt_data['detailed_results']:
                        best_result = opt_data['detailed_results'][0]['text']
                        print(f"  ğŸ’¡ æœ€ä½³åŒ¹é…: {best_result[:80]}...")
                else:
                    print(f"  âŒ ä¼˜åŒ–æœç´¢å¤±è´¥: {optimized_response.text}")
                    print(f"  ğŸ“ˆ ä¼˜åŒ–æœç´¢å¹³å‡å¾—åˆ†: 0.0000")
                    print(f"  ğŸš€ ç²¾åº¦æå‡: -100.00%")
            else:
                print(f"  âŒ ä¼ ç»Ÿæœç´¢å¤±è´¥: {traditional_response.text}")
                if optimized_response.status_code == 200:
                    print(f"  ğŸ“ˆ ä¼˜åŒ–æœç´¢æˆåŠŸï¼Œä½†æ— æ³•å¯¹æ¯”")
                else:
                    print(f"  âŒ ä¼˜åŒ–æœç´¢ä¹Ÿå¤±è´¥: {optimized_response.text}")

        print()

    def run_benchmark(self):
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        print("ğŸ“Š è¿è¡Œæœç´¢è´¨é‡åŸºå‡†æµ‹è¯•")
        print("=" * 50)

        benchmark_response = requests.post(
            f"{self.base_url}/benchmark_search_quality",
            json={"queries": self.test_queries}
        )

        if benchmark_response.status_code == 200:
            results = benchmark_response.json()
            print(f"âœ… åŸºå‡†æµ‹è¯•å®Œæˆ")

            # å®‰å…¨åœ°è·å–å„ç§å­—æ®µ
            if 'test_queries_count' in results:
                print(f"æµ‹è¯•æŸ¥è¯¢æ•°é‡: {results['test_queries_count']}")
            if 'overall_improvement' in results:
                print(f"æ€»ä½“ç²¾åº¦æå‡: {results['overall_improvement']:.4f}")
            if 'optimization_enabled' in results:
                print(f"ä¼˜åŒ–åŠŸèƒ½çŠ¶æ€: {'âœ… å·²å¯ç”¨' if results['optimization_enabled'] else 'âŒ æœªå¯ç”¨'}")

            # è¯¦ç»†ç»“æœ
            if 'detailed_results' in results:
                print("\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
                for result in results['detailed_results']:
                    query = result.get('query', 'Unknown')[:30] + '...'
                    trad_score = result.get('traditional_avg_score', 0)
                    opt_score = result.get('optimized_avg_score', 0)
                    improvement = result.get('improvement', 0)

                    print(f"  æŸ¥è¯¢: {query}")
                    print(f"  ä¼ ç»Ÿå¾—åˆ†: {trad_score:.4f}")
                    print(f"  ä¼˜åŒ–å¾—åˆ†: {opt_score:.4f}")
                    print(f"  æ”¹è¿›å¹…åº¦: {improvement:+.4f}")
                    print()
            else:
                print("\nğŸ“Š åŸºå‡†æµ‹è¯•å“åº”ç»“æ„:")
                print(f"  å¯ç”¨å­—æ®µ: {list(results.keys())}")
        else:
            print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {benchmark_response.text}")
            print(f"   é”™è¯¯ä»£ç : {benchmark_response.status_code}")
            print("   å»ºè®®æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—æˆ–APIå®ç°")

    def get_search_recommendations(self):
        """è·å–æœç´¢ä¼˜åŒ–å»ºè®®"""
        print("ğŸ’¡ æœç´¢ä¼˜åŒ–å»ºè®®")
        print("=" * 30)

        response = requests.get(f"{self.base_url}/search_recommendations")

        if response.status_code == 200:
            recommendations = response.json()

            # è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯
            stats_response = requests.get(f"{self.base_url}/stats")
            optimization_enabled = False
            current_vectors = 0
            current_index_type = "Unknown"

            if stats_response.status_code == 200:
                stats = stats_response.json()
                optimization_enabled = stats.get('search_optimization_enabled', False)
                current_vectors = stats.get('total_vectors', 0)
                current_index_type = stats.get('index_type', 'Unknown')

            print(f"å½“å‰å‘é‡æ•°é‡: {current_vectors}")
            print(f"å½“å‰ç´¢å¼•ç±»å‹: {current_index_type}")
            print(f"ä¼˜åŒ–åŠŸèƒ½çŠ¶æ€: {'âœ… å·²å¯ç”¨' if optimization_enabled else 'âŒ æœªå¯ç”¨'}")

            # æ˜¾ç¤ºå½“å‰é…ç½®
            if 'current_config' in recommendations:
                config = recommendations['current_config']
                print("\nâš™ï¸ å½“å‰é…ç½®:")
                if 'ivf_nprobe' in config:
                    print(f"  IVFæ¢æµ‹æ•°é‡: {config['ivf_nprobe']}")
                if 'hnsw_ef_search' in config:
                    print(f"  HNSWæœç´¢å‚æ•°: {config['hnsw_ef_search']}")
                if 'diversity_weight' in config:
                    print(f"  å¤šæ ·æ€§æƒé‡: {config['diversity_weight']}")
                if 'relevance_threshold' in config:
                    print(f"  ç›¸å…³æ€§é˜ˆå€¼: {config['relevance_threshold']}")

            # æ˜¾ç¤ºå»ºè®®ç´¢å¼•ç±»å‹
            if 'suggested_index_type' in recommendations:
                print(f"\nğŸ¯ å»ºè®®ç´¢å¼•ç±»å‹: {recommendations['suggested_index_type']}")

            print("\nğŸ”§ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(recommendations['recommendations'], 1):
                print(f"  {i}. {rec}")
        else:
            print(f"âŒ è·å–å»ºè®®å¤±è´¥: {response.text}")

    def get_system_stats(self):
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ“ˆ ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯")
        print("=" * 30)

        response = requests.get(f"{self.base_url}/stats")

        if response.status_code == 200:
            stats = response.json()
            print(f"æ€»å‘é‡æ•°é‡: {stats['total_vectors']}")
            print(f"åµŒå…¥ç»´åº¦: {stats['embedding_dim']}")
            print(f"ç´¢å¼•ç±»å‹: {stats['index_type']}")
            print(f"æ¨¡å‹åç§°: {stats['model_name']}")
            print(f"æœç´¢ä¼˜åŒ–: {'âœ… å·²å¯ç”¨' if stats['search_optimization_enabled'] else 'âŒ æœªå¯ç”¨'}")

            # ç´¢å¼•ç‰¹å®šä¿¡æ¯
            if 'is_trained' in stats:
                print(f"ç´¢å¼•è®­ç»ƒçŠ¶æ€: {'âœ… å·²è®­ç»ƒ' if stats['is_trained'] else 'âŒ æœªè®­ç»ƒ'}")
            if 'nlist' in stats:
                print(f"IVFèšç±»æ•°é‡: {stats['nlist']}")
            if 'nprobe' in stats:
                print(f"IVFæ¢æµ‹æ•°é‡: {stats['nprobe']}")
        else:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {response.text}")

    def enable_optimization(self):
        """å¯ç”¨æœç´¢ä¼˜åŒ–"""
        print("ğŸš€ æ­£åœ¨å¯ç”¨æœç´¢ä¼˜åŒ–...")

        response = requests.post(f"{self.base_url}/enable_optimization")

        if response.status_code == 200:
            result = response.json()
            print(f"âœ… {result['message']}")
            print("\nğŸ¯ å¯ç”¨çš„åŠŸèƒ½:")
            for feature in result['features']:
                print(f"  â€¢ {feature}")
        else:
            print(f"âŒ å¯ç”¨ä¼˜åŒ–å¤±è´¥: {response.text}")

    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸ¯ FAISSæœç´¢ä¼˜åŒ–å®Œæ•´æ¼”ç¤º")
        print("=" * 50)

        try:
            # 1. æ·»åŠ ç¤ºä¾‹æ–‡æ¡£
            self.add_sample_documents()

            # 2. è·å–åˆå§‹ç»Ÿè®¡ä¿¡æ¯
            self.get_system_stats()

            # 3. è·å–ä¼˜åŒ–å»ºè®®
            self.get_search_recommendations()

            # 4. å¯ç”¨ä¼˜åŒ–åŠŸèƒ½
            self.enable_optimization()

            # 5. å¯¹æ¯”æœç´¢æ–¹æ³•
            self.compare_search_methods()

            # 6. è¿è¡ŒåŸºå‡†æµ‹è¯•
            self.run_benchmark()

            # 7. æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
            print("\nğŸ“‹ æœ€ç»ˆç³»ç»ŸçŠ¶æ€:")
            self.get_system_stats()

            print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")

        except requests.exceptions.ConnectionError:
            print("âŒ æ— æ³•è¿æ¥åˆ°FAISSæœåŠ¡å™¨ï¼Œè¯·ç¡®ä¿æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")
        except Exception as e:
            print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    def test_chunk_effectiveness(self):
        """æµ‹è¯•åˆ†å—æ•ˆæœ"""
        print("ğŸ”§ æµ‹è¯•æ–‡æ¡£åˆ†å—æ•ˆæœ")
        print("=" * 30)

        # æ·»åŠ ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£å¹¶æ£€æŸ¥åˆ†å—ç»“æœ
        test_doc = self.sample_docs[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡æ¡£ä½œä¸ºæµ‹è¯•

        response = requests.post(
            f"{self.base_url}/add",
            json={
                "content": test_doc,
                "chunk_size": 500,
                "chunk_overlap": 100,
                "return_chunks": True  # å‡è®¾APIæ”¯æŒè¿”å›åˆ†å—ä¿¡æ¯
            }
        )

        if response.status_code == 200:
            result = response.json()
            if 'chunks' in result:
                print(f"âœ… åŸå§‹æ–‡æ¡£é•¿åº¦: {len(test_doc)} å­—ç¬¦")
                print(f"âœ… ç”Ÿæˆåˆ†å—æ•°é‡: {len(result['chunks'])}")
                print("\nğŸ“‹ åˆ†å—è¯¦æƒ…:")
                for i, chunk in enumerate(result['chunks']):
                    print(f"  åˆ†å— {i+1}: {len(chunk)} å­—ç¬¦")
                    print(f"  å†…å®¹é¢„è§ˆ: {chunk[:100]}...")
                    print()
            else:
                print("âœ… æ–‡æ¡£æ·»åŠ æˆåŠŸï¼Œä½†æœªè¿”å›åˆ†å—è¯¦æƒ…")
                print(f"ğŸ“Š åŸå§‹æ–‡æ¡£é•¿åº¦: {len(test_doc)} å­—ç¬¦")
        else:
            print(f"âŒ æµ‹è¯•åˆ†å—å¤±è´¥: {response.text}")

    def analyze_document_quality(self):
        """åˆ†ææ–‡æ¡£è´¨é‡"""
        print("ğŸ“Š åˆ†æç¤ºä¾‹æ–‡æ¡£è´¨é‡")
        print("=" * 30)

        for i, doc in enumerate(self.sample_docs):
            char_count = len(doc)
            word_count = len(doc.replace('ï¼Œ', ' ').replace('ã€‚', ' ').replace('ï¼', ' ').replace('ï¼Ÿ', ' ').split())
            sentence_count = doc.count('ã€‚') + doc.count('ï¼') + doc.count('ï¼Ÿ') + doc.count('.')

            print(f"ğŸ“„ æ–‡æ¡£ {i+1} ({doc.split('ï¼Œ')[0]}...):")
            print(f"  å­—ç¬¦æ•°: {char_count}")
            print(f"  è¯æ•°: {word_count}")
            print(f"  å¥å­æ•°: {sentence_count}")
            print(f"  é¢„è®¡åˆ†å—æ•°: {max(1, char_count // 400)}")  # å‡è®¾400å­—ç¬¦/åˆ†å—
            print()

        total_chars = sum(len(doc) for doc in self.sample_docs)
        avg_chars = total_chars / len(self.sample_docs)

        print(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»æ–‡æ¡£æ•°: {len(self.sample_docs)}")
        print(f"  æ€»å­—ç¬¦æ•°: {total_chars}")
        print(f"  å¹³å‡æ–‡æ¡£é•¿åº¦: {avg_chars:.1f} å­—ç¬¦")
        print(f"  é¢„è®¡æ€»åˆ†å—æ•°: {max(10, total_chars // 400)}")

if __name__ == "__main__":
    print("FAISSæœç´¢ä¼˜åŒ–æ¼”ç¤ºå·¥å…·")
    print("è¯·ç¡®ä¿FAISSæœåŠ¡å™¨æ­£åœ¨ http://localhost:8001 è¿è¡Œ")
    print()

    demo = SearchOptimizationDemo()

    # å…ˆåˆ†ææ–‡æ¡£è´¨é‡
    demo.analyze_document_quality()
    print()

    # è¿è¡Œå®Œæ•´æ¼”ç¤º
    demo.run_complete_demo()